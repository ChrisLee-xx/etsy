"""
Real Chrome Scraper - ä½¿ç”¨ Selenium è¿æ¥çœŸå® Chrome

æ ¸å¿ƒæ€è·¯ï¼š
1. é¦–å…ˆè®©ä½ åœ¨çœŸå®çš„ Chrome ä¸­æ‰‹åŠ¨è®¿é—® Etsy å¹¶å®ŒæˆéªŒè¯
2. ç„¶å Selenium è¿æ¥åˆ°è¯¥æµè§ˆå™¨è¿›è¡Œæ•°æ®æå–
3. è¿™æ ·å¯ä»¥å¤ç”¨ä½ æ‰‹åŠ¨éªŒè¯åçš„ä¼šè¯
"""
import json
import os
import random
import re
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List

import requests


def sanitize_filename(name: str, max_length: int = 100) -> str:
    """æ¸…ç†æ–‡ä»¶å"""
    if not name:
        return "unnamed"
    sanitized = re.sub(r'[<>:"/\\|?*\n\r\t]', '_', name)
    return sanitized[:max_length].rstrip(' ._') or "unnamed"


def get_chrome_path() -> Optional[str]:
    """è·å– Chrome è·¯å¾„"""
    if sys.platform == "darwin":
        paths = [
            "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
            "/Applications/Chromium.app/Contents/MacOS/Chromium",
        ]
    elif sys.platform == "win32":
        paths = [
            os.path.expandvars(r"%ProgramFiles%\Google\Chrome\Application\chrome.exe"),
            os.path.expandvars(r"%LocalAppData%\Google\Chrome\Application\chrome.exe"),
        ]
    else:
        paths = ["/usr/bin/google-chrome", "/usr/bin/chromium-browser"]
    
    for p in paths:
        if os.path.exists(p):
            return p
    return None


def start_chrome_with_debug(url: str, port: int = 9222) -> subprocess.Popen:
    """å¯åŠ¨å¸¦è°ƒè¯•ç«¯å£çš„ Chrome"""
    chrome_path = get_chrome_path()
    if not chrome_path:
        raise RuntimeError("æ‰¾ä¸åˆ° Chromeï¼")
    
    # åˆ›å»ºä¸´æ—¶ç”¨æˆ·ç›®å½•é¿å…ä¸ç°æœ‰ Chrome å†²çª
    temp_user_dir = Path.home() / ".etsy_scraper_chrome_profile"
    temp_user_dir.mkdir(exist_ok=True)
    
    cmd = [
        chrome_path,
        f"--remote-debugging-port={port}",
        f"--user-data-dir={temp_user_dir}",
        "--no-first-run",
        "--no-default-browser-check",
        "--disable-blink-features=AutomationControlled",
        f"--window-size={random.randint(1200, 1920)},{random.randint(800, 1080)}",
        url
    ]
    
    return subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)


def wait_for_chrome_ready(port: int = 9222, timeout: int = 30) -> bool:
    """ç­‰å¾… Chrome è°ƒè¯•ç«¯å£å°±ç»ª"""
    import time
    start = time.time()
    while time.time() - start < timeout:
        try:
            resp = requests.get(f"http://localhost:{port}/json", timeout=2)
            if resp.status_code == 200:
                return True
        except:
            pass
        time.sleep(1)
    return False


def extract_data_with_selenium(port: int = 9222) -> Optional[Dict]:
    """ä½¿ç”¨ Selenium è¿æ¥å¹¶æå–æ•°æ®"""
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    
    options = Options()
    options.add_experimental_option("debuggerAddress", f"localhost:{port}")
    
    driver = webdriver.Chrome(options=options)
    
    try:
        # è·å–å½“å‰ URL
        current_url = driver.current_url
        print(f"å½“å‰é¡µé¢: {current_url}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ Etsy äº§å“é¡µé¢
        # æ–¹æ³•ï¼šçœ‹æ˜¯å¦èƒ½æ‰¾åˆ°äº§å“é¡µé¢çš„å…³é”®å…ƒç´ ï¼Œè€Œä¸æ˜¯æ£€æµ‹"éªŒè¯"å…³é”®è¯
        is_product_page = False
        
        # å°è¯•æŸ¥æ‰¾äº§å“é¡µé¢ç‰¹æœ‰çš„å…ƒç´ 
        product_indicators = [
            'h1[data-buy-box-listing-title="true"]',  # äº§å“æ ‡é¢˜
            'div[data-appears-component-name="listing_page"]',  # äº§å“é¡µé¢æ ‡è®°
            'div.listing-page-image-carousel',  # å›¾ç‰‡è½®æ’­
            'button[data-add-to-cart-button]',  # åŠ å…¥è´­ç‰©è½¦æŒ‰é’®
            'div[data-buy-box-region="price"]',  # ä»·æ ¼åŒºåŸŸ
        ]
        
        for selector in product_indicators:
            try:
                el = driver.find_element(By.CSS_SELECTOR, selector)
                if el:
                    is_product_page = True
                    print(f"âœ“ æ£€æµ‹åˆ°äº§å“é¡µé¢å…ƒç´ : {selector}")
                    break
            except:
                continue
        
        # å¤‡ç”¨æ£€æµ‹ï¼šçœ‹ URL æ˜¯å¦åŒ…å« listing ä¸”é¡µé¢æœ‰ h1 æ ‡é¢˜
        if not is_product_page:
            if '/listing/' in current_url:
                try:
                    h1 = driver.find_element(By.TAG_NAME, 'h1')
                    h1_text = h1.text.strip()
                    # ç¡®ä¿ h1 ä¸æ˜¯éªŒè¯é¡µé¢çš„æ ‡é¢˜
                    if h1_text and len(h1_text) > 5 and 'éªŒè¯' not in h1_text and 'robot' not in h1_text.lower():
                        is_product_page = True
                        print(f"âœ“ æ£€æµ‹åˆ°äº§å“æ ‡é¢˜: {h1_text[:50]}...")
                except:
                    pass
        
        if not is_product_page:
            print("âš ï¸  æœªæ£€æµ‹åˆ°äº§å“é¡µé¢å…ƒç´ ï¼")
            print("   å¯èƒ½åŸå› ï¼š")
            print("   1. è¿˜åœ¨éªŒè¯é¡µé¢")
            print("   2. é¡µé¢æœªå®Œå…¨åŠ è½½")
            print("   3. ä¸æ˜¯æœ‰æ•ˆçš„ Etsy äº§å“é¡µé¢")
            
            # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦å¼ºåˆ¶ç»§ç»­
            force = input("\næ˜¯å¦å¼ºåˆ¶ç»§ç»­æŠ“å–? (y/N): ")
            if force.lower() != 'y':
                return None
            print("å¼ºåˆ¶ç»§ç»­...")
        
        # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨
        print("æ¨¡æ‹Ÿæµè§ˆè¡Œä¸º...")
        for _ in range(3):
            scroll_distance = random.randint(200, 500)
            driver.execute_script(f"window.scrollBy(0, {scroll_distance})")
            time.sleep(random.uniform(0.5, 1.5))
        
        # æ»šåŠ¨å›é¡¶éƒ¨
        driver.execute_script("window.scrollTo(0, 0)")
        time.sleep(1)
        
        # æå–æ•°æ®
        print("æå–æ•°æ®...")
        
        data = {}
        
        # æ ‡é¢˜
        try:
            title_el = driver.find_element(By.CSS_SELECTOR, 'h1[data-buy-box-listing-title="true"]')
            data['title'] = title_el.text.strip()
        except:
            try:
                title_el = driver.find_element(By.TAG_NAME, 'h1')
                data['title'] = title_el.text.strip()
            except:
                data['title'] = None
        
        # åº—é“º
        try:
            shop_link = driver.find_element(By.CSS_SELECTOR, 'a[href*="/shop/"]')
            href = shop_link.get_attribute('href')
            match = re.search(r'/shop/([^/?]+)', href)
            data['shop_name'] = match.group(1) if match else None
        except:
            data['shop_name'] = None
        
        # ä»·æ ¼
        try:
            price_el = driver.find_element(By.CSS_SELECTOR, 'span.currency-value')
            data['price'] = price_el.text.strip()
        except:
            data['price'] = None
        
        # å›¾ç‰‡ - åªè·å–å•†å“è¯¦æƒ…ä¸»å›¾ï¼Œæ’é™¤ "More from this shop" ç­‰æ‚å›¾
        images = []
        seen_ids = set()  # ä½¿ç”¨å›¾ç‰‡IDå»é‡ï¼Œè€Œä¸æ˜¯URL
        
        def extract_image_id(url):
            """ä» URL ä¸­æå–å›¾ç‰‡å”¯ä¸€ IDï¼Œå¦‚ 7261901436"""
            # åŒ¹é… il_xxxxx.æ•°å­—ID_åç¼€.jpg æ ¼å¼
            match = re.search(r'/il_[^.]+\.(\d+)_', url)
            return match.group(1) if match else None
        
        def convert_to_fullsize(url):
            """å°†ä»»ä½•å°ºå¯¸çš„å›¾ç‰‡ URL è½¬æ¢ä¸ºå…¨å°ºå¯¸"""
            # åŒ¹é…å„ç§å°ºå¯¸æ ¼å¼: _794xN, _570xN, _1588xN, _fullxfull, _300x300 ç­‰
            return re.sub(r'il_[^.]+\.', 'il_fullxfull.', url)
        
        # æ–¹æ³•0ï¼ˆæœ€ä¼˜å…ˆï¼‰: ç›´æ¥ä» data-src-zoom-image å±æ€§è·å–æœ€é«˜æ¸…å›¾ç‰‡
        # è¿™ä¸ªå±æ€§ç›´æ¥åŒ…å« fullxfull ç‰ˆæœ¬çš„ URLï¼Œæ— éœ€è½¬æ¢
        try:
            zoom_imgs = driver.find_elements(
                By.CSS_SELECTOR, 
                'li[data-carousel-pane]:not([data-video-pane]) img[data-src-zoom-image]'
            )
            for img in zoom_imgs:
                zoom_url = img.get_attribute('data-src-zoom-image')
                if zoom_url and 'etsystatic.com' in zoom_url:
                    img_id = extract_image_id(zoom_url)
                    if img_id and img_id not in seen_ids:
                        seen_ids.add(img_id)
                        images.append(zoom_url)
            if images:
                print(f"  âœ“ ä» data-src-zoom-image ç›´æ¥è·å– {len(images)} å¼ é«˜æ¸…ä¸»å›¾")
        except Exception as e:
            print(f"  æ–¹æ³•0å¤±è´¥: {e}")
        
        # æ–¹æ³•1: ä»äº§å“å›¾ç‰‡è½®æ’­/ç”»å»ŠåŒºåŸŸè·å–ï¼ˆå¤‡é€‰ï¼‰
        if not images:
            gallery_selectors = [
                # ä¸»å›¾ç‰‡ç”»å»Šå®¹å™¨
                'div[data-component="listing-page-image-carousel"] img',
                'ul[data-carousel-pagination-list] img',
                'div.image-carousel-container img',
                'div.listing-page-image-carousel img',
                # ç¼©ç•¥å›¾åˆ—è¡¨
                'ul.carousel-pane-list img[src*="il_"]',
                'div[data-appears-component-name="image_carousel"] img',
            ]
            
            for selector in gallery_selectors:
                try:
                    gallery_imgs = driver.find_elements(By.CSS_SELECTOR, selector)
                    for img in gallery_imgs:
                        src = img.get_attribute('src') or img.get_attribute('data-src')
                        if src and 'il_' in src and 'etsystatic.com' in src:
                            img_id = extract_image_id(src)
                            if img_id and img_id not in seen_ids:
                                seen_ids.add(img_id)
                                full_src = convert_to_fullsize(src)
                                images.append(full_src)
                    if images:
                        print(f"  âœ“ ä»ç”»å»ŠåŒºåŸŸæ‰¾åˆ° {len(images)} å¼ ä¸»å›¾")
                        break
                except:
                    continue
        
        # æ–¹æ³•2: å¦‚æœå‰é¢æ–¹æ³•æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ JavaScript ä»é¡µé¢é¡¶éƒ¨åŒºåŸŸæå–
        if not images:
            try:
                js_images = driver.execute_script('''
                    const images = [];
                    const seenIds = new Set();
                    
                    function extractImageId(url) {
                        const match = url.match(/\\/il_[^.]+\\.(\\d+)_/);
                        return match ? match[1] : null;
                    }
                    
                    function convertToFullsize(url) {
                        return url.replace(/il_[^.]+\\./, 'il_fullxfull.');
                    }
                    
                    // è·å–äº§å“å›¾ç‰‡åŒºåŸŸï¼ˆé€šå¸¸åœ¨é¡µé¢å·¦ä¾§/é¡¶éƒ¨ï¼‰
                    const containers = document.querySelectorAll([
                        '[data-component*="image"]',
                        '[class*="listing-page-image"]',
                        '[class*="image-carousel"]',
                        '[data-appears-component-name*="image"]'
                    ].join(','));
                    
                    containers.forEach(container => {
                        container.querySelectorAll('img').forEach(img => {
                            let src = img.src || img.dataset.src;
                            if (src && src.includes('etsystatic.com') && src.includes('/il_')) {
                                const imgId = extractImageId(src);
                                if (imgId && !seenIds.has(imgId)) {
                                    seenIds.add(imgId);
                                    images.push(convertToFullsize(src));
                                }
                            }
                        });
                    });
                    
                    return images;
                ''')
                
                if js_images:
                    for src in js_images:
                        img_id = extract_image_id(src)
                        if img_id and img_id not in seen_ids:
                            seen_ids.add(img_id)
                            images.append(src)
                    print(f"  âœ“ é€šè¿‡JSä»å›¾ç‰‡åŒºåŸŸæ‰¾åˆ° {len(images)} å¼ ä¸»å›¾")
            except Exception as e:
                print(f"  JSæå–å¤±è´¥: {e}")
        
        # æ–¹æ³•3: æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ - åªè·å–å¸¦æœ‰ç‰¹å®š listing ID çš„å›¾ç‰‡
        if not images:
            try:
                # ä» URL è·å– listing ID
                listing_match = re.search(r'/listing/(\d+)/', current_url)
                if listing_match:
                    all_imgs = driver.find_elements(By.CSS_SELECTOR, 'img[src*="etsystatic.com/il_"]')
                    
                    for img in all_imgs:
                        src = img.get_attribute('src')
                        if src:
                            # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åœ¨é¡µé¢ä¸ŠåŠéƒ¨åˆ†ï¼ˆäº§å“è¯¦æƒ…åŒºåŸŸï¼‰
                            try:
                                location = img.location
                                # åªè·å–é¡µé¢ä¸Šéƒ¨çš„å›¾ç‰‡ï¼ˆy < 1500 åƒç´ ï¼‰
                                if location['y'] < 1500:
                                    img_id = extract_image_id(src)
                                    if img_id and img_id not in seen_ids:
                                        seen_ids.add(img_id)
                                        full_src = convert_to_fullsize(src)
                                        images.append(full_src)
                            except:
                                pass
                    
                    if images:
                        print(f"  âœ“ é€šè¿‡ä½ç½®è¿‡æ»¤æ‰¾åˆ° {len(images)} å¼ ä¸»å›¾")
            except:
                pass
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡ï¼ˆä¸€èˆ¬å•†å“ä¸»å›¾ä¸ä¼šè¶…è¿‡10å¼ ï¼‰
        images = list(dict.fromkeys(images))[:15]
        print(f"  æœ€ç»ˆè·å– {len(images)} å¼ å•†å“ä¸»å›¾")
        
        data['images'] = images
        
        # äº§å“ ID
        product_id_match = re.search(r'/listing/(\d+)/', current_url)
        data['product_id'] = product_id_match.group(1) if product_id_match else None
        
        data['url'] = current_url
        data['scraped_at'] = datetime.now().isoformat()
        
        return data
        
    finally:
        # ä¸è¦å…³é—­æµè§ˆå™¨ï¼Œè®©ç”¨æˆ·å¯ä»¥ç»§ç»­ä½¿ç”¨
        pass


def download_images(images: List[str], title: str, output_dir: Path, 
                    image_selection: List[int] = None, filter_words: List[str] = None):
    """ä¸‹è½½å›¾ç‰‡
    
    Args:
        images: å›¾ç‰‡ URL åˆ—è¡¨
        title: å•†å“æ ‡é¢˜
        output_dir: è¾“å‡ºç›®å½•
        image_selection: è¦ä¸‹è½½çš„å›¾ç‰‡åºå·åˆ—è¡¨ï¼ˆ1-indexedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        filter_words: ä»æ ‡é¢˜ä¸­è¿‡æ»¤çš„è¯æ±‡åˆ—è¡¨
    """
    if not images or not title:
        return
    
    # åº”ç”¨æ ‡é¢˜è¿‡æ»¤
    try:
        from .utils import filter_title
    except ImportError:
        from utils import filter_title
    display_title = title
    if filter_words:
        display_title = filter_title(title, filter_words)
    
    safe_title = sanitize_filename(display_title)
    
    # ç¡®å®šè¦ä¸‹è½½çš„å›¾ç‰‡
    if image_selection:
        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„åºå·
        valid_indices = [i for i in image_selection if 1 <= i <= len(images)]
        skipped_indices = [i for i in image_selection if i > len(images)]
        
        if skipped_indices:
            print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„å›¾ç‰‡åºå·: {skipped_indices} (å…± {len(images)} å¼ å›¾ç‰‡)")
        
        if not valid_indices:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡åºå·å¯ä¸‹è½½")
            return
        
        download_list = [(i, images[i-1]) for i in valid_indices]
        print(f"\nä¸‹è½½ {len(download_list)}/{len(images)} å¼ å›¾ç‰‡ (åºå·: {valid_indices})...")
    else:
        download_list = [(i+1, url) for i, url in enumerate(images)]
        print(f"\nä¸‹è½½ {len(images)} å¼ å›¾ç‰‡...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
        "Referer": "https://www.etsy.com/"
    }
    
    for idx, url in download_list:
        try:
            ext = url.split('.')[-1].split('?')[0] or 'jpg'
            filename = f"{safe_title}-{idx}.{ext}"
            filepath = output_dir / filename
            
            resp = requests.get(url, headers=headers, timeout=30)
            if resp.status_code == 200:
                filepath.write_bytes(resp.content)
                print(f"  âœ“ [{idx}/{len(images)}] {filename}")
            else:
                print(f"  âœ— [{idx}/{len(images)}] HTTP {resp.status_code}")
        except Exception as e:
            print(f"  âœ— [{idx}/{len(images)}] {e}")
        
        time.sleep(random.uniform(0.3, 0.8))


def main():
    """ä¸»å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨çœŸå® Chrome çš„ Etsy çˆ¬è™« - æœ€å¼ºåçˆ¬è™«æ–¹æ¡ˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•ä¸ªé“¾æ¥
  etsy-real "https://www.etsy.com/listing/123456789"
  
  # å¤šä¸ªé“¾æ¥ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
  etsy-real "https://www.etsy.com/listing/111" "https://www.etsy.com/listing/222"
  
  # å¤šä¸ªé“¾æ¥ + é€‰é¡¹
  etsy-real "https://www.etsy.com/listing/111" "https://www.etsy.com/listing/222" -i 1

å·¥ä½œæµç¨‹:
  1. å¯åŠ¨çœŸå® Chrome æµè§ˆå™¨ï¼ˆå¸¦è°ƒè¯•ç«¯å£ï¼‰
  2. ä½ åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯/ç™»å½•
  3. éªŒè¯å®Œæˆåï¼ŒæŒ‰ Enter ç»§ç»­
  4. è‡ªåŠ¨æå–æ•°æ®å’Œä¸‹è½½å›¾ç‰‡ï¼ˆå¤šé“¾æ¥ä¼šä¾æ¬¡å¤„ç†ï¼‰

ä¸ºä»€ä¹ˆè¿™æ ·èƒ½ç»•è¿‡åçˆ¬è™«?
  - ä½¿ç”¨çœŸå® Chromeï¼Œä¸æ˜¯è‡ªåŠ¨åŒ–æµè§ˆå™¨
  - ä½ æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œä¸éœ€è¦ç»•è¿‡
  - å¤ç”¨å·²éªŒè¯çš„ä¼šè¯çŠ¶æ€
"""
    )
    
    parser.add_argument("urls", nargs="+", help="Etsy äº§å“ URLï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    parser.add_argument("--output", "-o", default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--port", "-p", type=int, default=9222, help="è°ƒè¯•ç«¯å£")
    parser.add_argument("--delay", "-d", type=float, default=2.0, help="å¤šé“¾æ¥é—´å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 2ï¼‰")
    parser.add_argument("--images", "-i", default=None, 
                        help="æŒ‡å®šä¸‹è½½å“ªäº›å›¾ç‰‡ï¼Œå¦‚: '1' æˆ– '1,3,5' æˆ– '2-4' æˆ– '1,3-5,8'")
    parser.add_argument("--filter", "-f", default=None,
                        help="ä»æ ‡é¢˜ä¸­è¿‡æ»¤çš„è¯æ±‡ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚: 'Canvas,Poster,Wall Art'")
    
    args = parser.parse_args()
    
    # è§£æå›¾ç‰‡é€‰æ‹©å’Œè¿‡æ»¤è¯å‚æ•°
    try:
        from .utils import parse_image_selection, parse_filter_words
    except ImportError:
        from utils import parse_image_selection, parse_filter_words
    
    image_selection = None
    filter_words = None
    
    if args.images:
        try:
            image_selection = parse_image_selection(args.images)
            print(f"âœ“ å›¾ç‰‡é€‰æ‹©: {image_selection}")
        except ValueError as e:
            print(f"âŒ {e}")
            return
    
    if args.filter:
        filter_words = parse_filter_words(args.filter)
        print(f"âœ“ æ ‡é¢˜è¿‡æ»¤è¯: {filter_words}")
    
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    urls = args.urls
    total_urls = len(urls)
    
    print("\n" + "=" * 60)
    print("ğŸŒ REAL CHROME ETSY SCRAPER")
    print("=" * 60)
    print("è¿™ä¸ªçˆ¬è™«ä½¿ç”¨çœŸå®çš„ Chrome æµè§ˆå™¨")
    print("ä½ éœ€è¦æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œç„¶åè‡ªåŠ¨æŠ“å–æ•°æ®")
    if total_urls > 1:
        print(f"\nğŸ“‹ å…± {total_urls} ä¸ªé“¾æ¥å¾…å¤„ç†")
    print("=" * 60)
    
    # æ­¥éª¤ 1ï¼šå¯åŠ¨ Chrome
    print("\nğŸ“Œ æ­¥éª¤ 1: å¯åŠ¨ Chrome")
    print("-" * 40)
    print("âš ï¸  è¯·å…ˆå…³é—­æ‰€æœ‰ Chrome çª—å£ï¼")
    input("å‡†å¤‡å¥½åæŒ‰ Enter ç»§ç»­...")
    
    print("\nå¯åŠ¨ Chrome...")
    chrome_process = start_chrome_with_debug(urls[0], args.port)
    
    print("ç­‰å¾…æµè§ˆå™¨å°±ç»ª...")
    if not wait_for_chrome_ready(args.port):
        print("âŒ Chrome å¯åŠ¨å¤±è´¥ï¼")
        chrome_process.terminate()
        return
    
    print("âœ“ Chrome å·²å¯åŠ¨ï¼")
    
    # æ­¥éª¤ 2ï¼šç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯
    print("\n" + "=" * 60)
    print("ğŸ“Œ æ­¥éª¤ 2: å®ŒæˆéªŒè¯")
    print("-" * 40)
    print("""
åœ¨æ‰“å¼€çš„ Chrome çª—å£ä¸­ï¼š

  1. å¦‚æœçœ‹åˆ°éªŒè¯é¡µé¢ï¼Œè¯·å®Œæˆã€Œæˆ‘ä¸æ˜¯æœºå™¨äººã€éªŒè¯
  2. ç­‰å¾…äº§å“é¡µé¢å®Œå…¨åŠ è½½
  3. ä½ ä¹Ÿå¯ä»¥ç™»å½• Etsy è´¦å·ï¼ˆæ¨èï¼‰

â° æ²¡æœ‰æ—¶é—´é™åˆ¶ï¼Œæ…¢æ…¢æ¥ï¼
""")
    print("=" * 60)
    
    input("\nâœ‹ éªŒè¯å®Œæˆã€é¡µé¢åŠ è½½å¥½åï¼ŒæŒ‰ Enter ç»§ç»­...")
    
    # è¿æ¥ Selenium
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    
    options = Options()
    options.add_experimental_option("debuggerAddress", f"localhost:{args.port}")
    driver = webdriver.Chrome(options=options)
    
    # æ­¥éª¤ 3ï¼šå¤„ç†æ‰€æœ‰é“¾æ¥
    print("\nğŸ“Œ æ­¥éª¤ 3: æå–æ•°æ®")
    print("-" * 40)
    
    success_count = 0
    fail_count = 0
    
    for idx, url in enumerate(urls, 1):
        if total_urls > 1:
            print(f"\n{'='*60}")
            print(f"[{idx}/{total_urls}] å¤„ç†é“¾æ¥:")
            print(f"  {url}")
            print(f"{'='*60}")
        
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªé“¾æ¥ï¼Œéœ€è¦å¯¼èˆªåˆ°æ–°é¡µé¢
        if idx > 1:
            try:
                driver.get(url)
                # ç­‰å¾…é¡µé¢åŠ è½½
                time.sleep(2)
                # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨
                for _ in range(2):
                    scroll_distance = random.randint(200, 400)
                    driver.execute_script(f"window.scrollBy(0, {scroll_distance})")
                    time.sleep(random.uniform(0.3, 0.8))
                driver.execute_script("window.scrollTo(0, 0)")
                time.sleep(1)
            except Exception as e:
                print(f"  âŒ å¯¼èˆªå¤±è´¥: {e}")
                fail_count += 1
                continue
        
        result = extract_data_with_selenium(args.port)
        
        if not result or not result.get('title'):
            print(f"\nâŒ æŠ“å–å¤±è´¥ï¼")
            fail_count += 1
            if idx < total_urls:
                print("  ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªé“¾æ¥...")
            continue
        
        success_count += 1
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nâœ… æŠ“å–æˆåŠŸï¼")
        print(f"  æ ‡é¢˜: {result.get('title', 'N/A')}")
        print(f"  åº—é“º: {result.get('shop_name', 'N/A')}")
        print(f"  ä»·æ ¼: {result.get('price', 'N/A')}")
        print(f"  å›¾ç‰‡: {len(result.get('images', []))} å¼ ")
        
        # ä¿å­˜ JSON
        product_id = result.get('product_id', 'unknown')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = output_path / f"product_{product_id}_{timestamp}.json"
        json_path.write_text(json.dumps(result, indent=2, ensure_ascii=False))
        print(f"  âœ“ æ•°æ®å·²ä¿å­˜: {json_path}")
        
        # ä¸‹è½½å›¾ç‰‡
        download_images(result.get('images', []), result.get('title', ''), output_path,
                        image_selection=image_selection, filter_words=filter_words)
        
        # å¤šé“¾æ¥é—´å»¶è¿Ÿ
        if idx < total_urls:
            wait_time = args.delay + random.uniform(-0.5, 1.0)
            wait_time = max(1.0, wait_time)
            print(f"\nâ³ ç­‰å¾… {wait_time:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªé“¾æ¥...")
            time.sleep(wait_time)
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ‰ å®Œæˆï¼")
    print("=" * 60)
    if total_urls > 1:
        print(f"  æ€»é“¾æ¥æ•°: {total_urls}")
        print(f"  æˆåŠŸ: {success_count}")
        print(f"  å¤±è´¥: {fail_count}")
    print(f"  è¾“å‡ºç›®å½•: {output_path}")
    
    # è¯¢é—®æ˜¯å¦å…³é—­æµè§ˆå™¨
    close = input("\næ˜¯å¦å…³é—­ Chrome æµè§ˆå™¨? (y/N): ")
    if close.lower() == 'y':
        chrome_process.terminate()
        print("æµè§ˆå™¨å·²å…³é—­")
    else:
        print("æµè§ˆå™¨ä¿æŒæ‰“å¼€ï¼Œä½ å¯ä»¥ç»§ç»­ä½¿ç”¨")


if __name__ == "__main__":
    main()
