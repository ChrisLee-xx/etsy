"""
Section Scraper - æ‰¹é‡æŠ“å– Etsy åº—é“º Section ä¸‹çš„æ‰€æœ‰å•†å“å›¾ç‰‡

å·¥ä½œæµç¨‹ï¼š
1. è¾“å…¥ Section URL
2. è‡ªåŠ¨æå–è¯¥ Section ä¸‹æ‰€æœ‰å•†å“é“¾æ¥
3. ä¾æ¬¡è®¿é—®æ¯ä¸ªå•†å“é¡µé¢ï¼Œä¸‹è½½æ‰€æœ‰å›¾ç‰‡
4. æŒ‰æ‰å¹³ç›®å½•ç»“æ„ç»„ç»‡è¾“å‡º
"""
import argparse
import json
import math
import random
import re
import sys
import time
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import parse_qs, urlencode, urlparse, urlunparse

import requests


class ScrapeProgress:
    """
    ç®¡ç†æŠ“å–è¿›åº¦çš„æŒä¹…åŒ–
    
    è¿›åº¦æ–‡ä»¶ä½ç½®: {output_dir}/.progress.json
    
    è¿›åº¦æ–‡ä»¶æ ¼å¼:
    {
        "section_url": "https://...",
        "shop_name": "...",
        "section_id": "...",
        "started_at": "2026-02-03T10:00:00Z",
        "updated_at": "2026-02-03T10:30:00Z",
        "completed_ids": ["1234567890", ...],
        "total_found": 41
    }
    """
    
    def __init__(self, output_dir: Path, section_url: str, shop_name: str, section_id: str):
        """
        åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            section_url: Section URL
            shop_name: åº—é“ºåç§°
            section_id: Section ID
        """
        self.progress_file = output_dir / ".progress.json"
        self.section_url = section_url
        self.shop_name = shop_name
        self.section_id = section_id
        self._completed_ids: Set[str] = set()
        self._total_found: int = 0
        self._started_at: Optional[str] = None
    
    def load(self) -> Set[str]:
        """
        åŠ è½½å·²å®Œæˆçš„ listing_id é›†åˆ
        
        Returns:
            å·²å®Œæˆçš„ listing_id é›†åˆ
            
        Raises:
            ValueError: å¦‚æœè¿›åº¦æ–‡ä»¶æŸå
        """
        if not self.progress_file.exists():
            return set()
        
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self._completed_ids = set(data.get('completed_ids', []))
            self._total_found = data.get('total_found', 0)
            self._started_at = data.get('started_at')
            
            return self._completed_ids
            
        except json.JSONDecodeError as e:
            raise ValueError(
                f"è¿›åº¦æ–‡ä»¶æŸåï¼ŒJSON æ ¼å¼æ— æ•ˆ: {e}\n"
                f"è¯·åˆ é™¤æ–‡ä»¶åé‡è¯•: {self.progress_file}"
            )
        except Exception as e:
            raise ValueError(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    def save(self, completed_id: str):
        """
        ä¿å­˜æ–°å®Œæˆçš„ listing_id
        
        æ¯æ¬¡æˆåŠŸä¸‹è½½ä¸€ä¸ªå•†å“åè°ƒç”¨æ­¤æ–¹æ³•ï¼Œç«‹å³å†™å…¥æ–‡ä»¶
        
        Args:
            completed_id: åˆšå®Œæˆçš„å•†å“ listing_id
        """
        self._completed_ids.add(completed_id)
        
        now = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')
        
        if not self._started_at:
            self._started_at = now
        
        data = {
            "section_url": self.section_url,
            "shop_name": self.shop_name,
            "section_id": self.section_id,
            "started_at": self._started_at,
            "updated_at": now,
            "completed_ids": list(self._completed_ids),
            "total_found": self._total_found
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def set_total_found(self, total: int):
        """è®¾ç½®æ‰¾åˆ°çš„æ€»å•†å“æ•°"""
        self._total_found = total
    
    def is_completed(self, listing_id: str) -> bool:
        """
        æ£€æŸ¥ listing_id æ˜¯å¦å·²å®Œæˆ
        
        Args:
            listing_id: å•†å“ ID
            
        Returns:
            True å¦‚æœå·²å®Œæˆ
        """
        return listing_id in self._completed_ids
    
    def clear(self):
        """æ¸…ç†è¿›åº¦æ–‡ä»¶"""
        if self.progress_file.exists():
            self.progress_file.unlink()
            self._completed_ids = set()
            self._total_found = 0
            self._started_at = None
    
    @property
    def completed_count(self) -> int:
        """å·²å®Œæˆçš„å•†å“æ•°é‡"""
        return len(self._completed_ids)
    
    @property
    def total_found(self) -> int:
        """æ‰¾åˆ°çš„æ€»å•†å“æ•°"""
        return self._total_found

# å¤ç”¨ real_chrome_scraper çš„æ ¸å¿ƒå‡½æ•°
try:
    from .real_chrome_scraper import (
        sanitize_filename,
        get_chrome_path,
        start_chrome_with_debug,
        wait_for_chrome_ready,
        extract_data_with_selenium,
    )
except ImportError:
    from real_chrome_scraper import (
        sanitize_filename,
        get_chrome_path,
        start_chrome_with_debug,
        wait_for_chrome_ready,
        extract_data_with_selenium,
    )


def sanitize_folder_name(name: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶å¤¹åç§°ï¼Œæ›¿æ¢æ–‡ä»¶ç³»ç»Ÿéæ³•å­—ç¬¦
    
    Args:
        name: åŸå§‹åç§°
        
    Returns:
        å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°
    """
    # æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿéæ³•å­—ç¬¦ä¸º _
    unsafe_chars = r'/\:*?"<>|'
    result = name
    for char in unsafe_chars:
        result = result.replace(char, '_')
    # å»é™¤é¦–å°¾ç©ºç™½
    result = result.strip()
    # åˆå¹¶è¿ç»­ä¸‹åˆ’çº¿
    result = re.sub(r'_+', '_', result)
    # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    result = result.strip('_')
    return result


def parse_section_url(url: str) -> Tuple[str, str]:
    """
    è§£æ Section URLï¼Œæå– shop_name å’Œ section_id
    
    æ”¯æŒæ ¼å¼:
    - https://www.etsy.com/shop/{shop_name}?section_id={section_id}
    - https://www.etsy.com/shop/{shop_name}?section_id={section_id}&ref=...
    
    Returns:
        Tuple[shop_name, section_id]
    
    Raises:
        ValueError: å¦‚æœ URL æ ¼å¼æ— æ•ˆ
    """
    parsed = urlparse(url)
    
    # æå– shop_name - ä»è·¯å¾„ /shop/{shop_name}
    path_match = re.search(r'/shop/([^/?]+)', parsed.path)
    if not path_match:
        raise ValueError(f"æ— æ•ˆçš„ Section URL: æ‰¾ä¸åˆ°åº—é“ºåç§°\nURL: {url}")
    shop_name = path_match.group(1)
    
    # æå– section_id - ä»æŸ¥è¯¢å‚æ•°
    query_params = parse_qs(parsed.query)
    if 'section_id' not in query_params:
        raise ValueError(f"æ— æ•ˆçš„ Section URL: æ‰¾ä¸åˆ° section_id\nURL: {url}")
    section_id = query_params['section_id'][0]
    
    return shop_name, section_id


def build_page_url(section_url: str, page: int) -> str:
    """
    æ„é€ å¸¦ page å‚æ•°çš„ Section é¡µé¢ URL
    
    Args:
        section_url: åŸå§‹ Section URL
        page: é¡µç ï¼ˆä» 1 å¼€å§‹ï¼‰
        
    Returns:
        å¸¦ page=N å‚æ•°çš„ URL
    """
    parsed = urlparse(section_url)
    query_params = parse_qs(parsed.query, keep_blank_values=True)
    query_params['page'] = [str(page)]
    # å°†å¤šå€¼å‚æ•°æ‰å¹³åŒ–ä¸ºå•å€¼
    new_query = urlencode({k: v[0] for k, v in query_params.items()})
    new_url = urlunparse((
        parsed.scheme, parsed.netloc, parsed.path,
        parsed.params, new_query, ''
    ))
    return new_url


def extract_product_links(driver, section_url: str, total_items: int = 0) -> List[str]:
    """
    ä» Section é¡µé¢æå–æ‰€æœ‰å•†å“é“¾æ¥ï¼ˆåŸºäº URL å‚æ•°ç¿»é¡µï¼‰
    
    ç¿»é¡µç­–ç•¥ï¼š
    1. å…ˆæŠ“å–ç¬¬ 1 é¡µï¼Œè·å–å®é™…æ¯é¡µå•†å“æ•° (items_per_page)
    2. ç»“åˆ total_items è®¡ç®—æ€»é¡µæ•°: ceil(total_items / items_per_page)
    3. ä»ç¬¬ 2 é¡µå¼€å§‹é€é¡µæ„é€  URL è®¿é—®
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        section_url: Section é¡µé¢ URL
        total_items: Section æ€»å•†å“æ•°ï¼ˆç”¨äºè®¡ç®—æ€»é¡µæ•°ï¼Œ0 åˆ™é€é¡µæ¢æµ‹ï¼‰
        
    Returns:
        å•†å“ listing_id åˆ—è¡¨
    """
    from selenium.webdriver.common.by import By
    
    all_listing_ids = []
    seen_ids = set()
    items_per_page = 0  # ä»ç¬¬ä¸€é¡µåŠ¨æ€è·å–
    total_pages = None
    current_page = 1
    
    print(f"\nğŸ“Š Section æ€»å•†å“æ•°: {total_items}" if total_items > 0 else "\nğŸ“Š æ€»å•†å“æ•°æœªçŸ¥ï¼Œå°†é€é¡µæ¢æµ‹")
    
    while True:
        # æ„é€ å½“å‰é¡µ URL
        page_url = build_page_url(section_url, current_page)
        
        if total_pages is not None:
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {current_page}/{total_pages} é¡µ...")
        else:
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ...")
        
        # å¯¼èˆªåˆ°å½“å‰é¡µ
        driver.get(page_url)
        time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
        
        # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
        scroll_page(driver)
        
        # æå–å½“å‰é¡µçš„å•†å“ listing_id
        try:
            product_cards = driver.find_elements(
                By.CSS_SELECTOR, 
                'div.v2-listing-card[data-listing-id]'
            )
            
            page_ids = []
            for card in product_cards:
                listing_id = card.get_attribute('data-listing-id')
                if listing_id and listing_id not in seen_ids:
                    seen_ids.add(listing_id)
                    page_ids.append(listing_id)
                    all_listing_ids.append(listing_id)
            
            print(f"  âœ“ æœ¬é¡µæ‰¾åˆ° {len(page_ids)} ä¸ªæ–°å•†å“")
            
            # å¦‚æœæœ¬é¡µæ— æ–°å•†å“ï¼Œåœæ­¢ç¿»é¡µ
            if not page_ids:
                print("  â†’ æœ¬é¡µæ— æ–°å•†å“ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # ç¬¬ä¸€é¡µæŠ“å–å®Œæˆåï¼ŒåŠ¨æ€è®¡ç®—æ¯é¡µå•†å“æ•°å’Œæ€»é¡µæ•°
            if current_page == 1 and total_items > 0:
                items_per_page = len(page_ids)
                total_pages = math.ceil(total_items / items_per_page)
                print(f"  ğŸ“Š æ¯é¡µ {items_per_page} ä¸ªå•†å“ï¼Œé¢„è®¡å…± {total_pages} é¡µ")
                
                # å¦‚æœåªæœ‰ 1 é¡µï¼Œç›´æ¥ç»“æŸ
                if total_pages <= 1:
                    print(f"  â†’ ä»… 1 é¡µï¼Œæ— éœ€ç¿»é¡µ")
                    break
            
        except Exception as e:
            print(f"  âœ— æå–å•†å“å¤±è´¥: {e}")
            break
        
        # æ£€æŸ¥æ˜¯å¦å·²åˆ°è¾¾æœ€åä¸€é¡µ
        if total_pages is not None:
            if current_page >= total_pages:
                print(f"  â†’ å·²åˆ°è¾¾æœ€åä¸€é¡µ ({current_page}/{total_pages})")
                break
        
        current_page += 1
        time.sleep(2)  # ç¿»é¡µé—´å»¶è¿Ÿ
    
    return all_listing_ids


def scroll_page(driver, scroll_times: int = 5):
    """
    æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        scroll_times: æ»šåŠ¨æ¬¡æ•°
    """
    for i in range(scroll_times):
        # éšæœºæ»šåŠ¨è·ç¦»
        scroll_distance = random.randint(300, 600)
        driver.execute_script(f"window.scrollBy(0, {scroll_distance})")
        time.sleep(random.uniform(0.3, 0.8))
    
    # æ»šåŠ¨åˆ°åº•éƒ¨ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
    time.sleep(1)
    
    # æ»šåŠ¨å›é¡¶éƒ¨
    driver.execute_script("window.scrollTo(0, 0)")
    time.sleep(0.5)


def get_section_info(driver, section_id: str = None) -> Tuple[str, int]:
    """
    è·å– Section åç§°å’Œæ€»å•†å“æ•°
    
    æ”¯æŒä¸¤ç§é¡µé¢å¸ƒå±€ï¼š
    - å¤§å±å¹• (lg): ä¾§è¾¹æ ç”¨ <li data-section-id="..."> åˆ—è¡¨
    - å°å±å¹• (xs): ä¸‹æ‹‰èœå•ç”¨ <button data-section-id="..."> æŒ‰é’®
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        section_id: å¯é€‰ï¼ŒæŒ‡å®šè¦æŸ¥æ‰¾çš„ section_id
        
    Returns:
        Tuple[section_name, total_items]
    """
    from selenium.webdriver.common.by import By
    
    section_name = "section"
    total_items = 0
    
    if section_id:
        # æ–¹æ³•1: å°å±å¹•ä¸‹æ‹‰èœå• - æŸ¥æ‰¾ button[data-section-id]
        try:
            button = driver.find_element(
                By.CSS_SELECTOR,
                f'button[data-section-id="{section_id}"]'
            )
            # æŒ‰é’®æ–‡æœ¬æ ¼å¼: "Canvas (41)"
            button_text = button.text.strip()
            # è§£æ "åç§° (æ•°é‡)" æ ¼å¼
            match = re.match(r'^(.+?)\s*\((\d+)\)\s*$', button_text)
            if match:
                section_name = match.group(1).strip()
                total_items = int(match.group(2))
                print(f"  âœ“ ä»ä¸‹æ‹‰èœå•è·å–: {section_name} ({total_items} ä»¶å•†å“)")
                return section_name, total_items
        except Exception as e:
            print(f"  æ–¹æ³•1 (button data-section-id) æœªåŒ¹é…: {type(e).__name__}")
        
        # æ–¹æ³•2: å¤§å±å¹•ä¾§è¾¹æ  - æŸ¥æ‰¾ li[data-section-id]
        try:
            tab = driver.find_element(
                By.CSS_SELECTOR,
                f'li[data-section-id="{section_id}"]'
            )
            spans = tab.find_elements(By.CSS_SELECTOR, 'span')
            if len(spans) >= 2:
                section_name = spans[0].text.strip()
                count_text = spans[1].text.strip()
                count_match = re.search(r'(\d+)', count_text)
                if count_match:
                    total_items = int(count_match.group(1))
                print(f"  âœ“ ä»ä¾§è¾¹æ è·å–: {section_name} ({total_items} ä»¶å•†å“)")
                return section_name, total_items
        except Exception as e:
            print(f"  æ–¹æ³•2 (li data-section-id) æœªåŒ¹é…: {type(e).__name__}")
    
    # æ–¹æ³•3: å°å±å¹• - ä»ä¸‹æ‹‰èœå•è§¦å‘æŒ‰é’®è·å–å½“å‰é€‰ä¸­é¡¹
    try:
        trigger = driver.find_element(
            By.CSS_SELECTOR,
            '.wt-menu__trigger .wt-menu__trigger__label'
        )
        trigger_text = trigger.text.strip()
        # æ ¼å¼: "Canvas (41)"
        match = re.match(r'^(.+?)\s*\((\d+)\)\s*$', trigger_text)
        if match:
            section_name = match.group(1).strip()
            total_items = int(match.group(2))
            print(f"  âœ“ ä»ä¸‹æ‹‰èœå•è§¦å‘å™¨è·å–: {section_name} ({total_items} ä»¶å•†å“)")
            return section_name, total_items
    except Exception as e:
        print(f"  æ–¹æ³•3 (menu trigger) æœªåŒ¹é…: {type(e).__name__}")
    
    # æ–¹æ³•4: æŸ¥æ‰¾é€‰ä¸­çš„ tabï¼ˆå¤§å±å¹•ä¾§è¾¹æ å¤‡ç”¨æ–¹æ¡ˆï¼‰
    selectors = [
        'li.wt-tab__item[aria-selected="true"]',
        'li.wt-tab__item.is-selected',
        'li[role="tab"][aria-selected="true"]',
        'li[role="tab"].is-selected',
    ]
    
    for selector in selectors:
        try:
            selected_tab = driver.find_element(By.CSS_SELECTOR, selector)
            if selected_tab:
                spans = selected_tab.find_elements(By.CSS_SELECTOR, 'span')
                if len(spans) >= 2:
                    section_name = spans[0].text.strip()
                    count_text = spans[1].text.strip()
                    count_match = re.search(r'(\d+)', count_text)
                    if count_match:
                        total_items = int(count_match.group(1))
                    print(f"  âœ“ ä»ä¾§è¾¹æ è·å–: {section_name} ({total_items} ä»¶å•†å“)")
                    return section_name, total_items
        except:
            continue
    
    print(f"  âš ï¸ æœªèƒ½è·å– Section ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
    
    return section_name, total_items


class ImageNameTracker:
    """
    è·Ÿè¸ªå›¾ç‰‡å‘½åï¼Œå¤„ç†åŒåå•†å“
    
    åŒåå•†å“å¤„ç†è§„åˆ™ï¼š
    - ç¬¬ä¸€ä¸ªå•†å“: poster-1.jpg, poster-2.jpg
    - ç¬¬äºŒä¸ªåŒå: poster-1(1).jpg, poster-2(1).jpg
    - ç¬¬ä¸‰ä¸ªåŒå: poster-1(2).jpg, poster-2(2).jpg
    """
    
    def __init__(self):
        # è®°å½•æ¯ä¸ªå•†å“åç§°å‡ºç°çš„æ¬¡æ•°
        self.name_counts: Dict[str, int] = defaultdict(int)
    
    def get_suffix(self, product_name: str) -> str:
        """
        è·å–æ–‡ä»¶ååç¼€
        
        Args:
            product_name: å•†å“æ ‡é¢˜ï¼ˆæ¸…ç†åçš„ï¼‰
            
        Returns:
            åç¼€å­—ç¬¦ä¸²ï¼Œå¦‚ "" æˆ– "(1)" æˆ– "(2)"
        """
        count = self.name_counts[product_name]
        self.name_counts[product_name] += 1
        
        if count == 0:
            return ""
        else:
            return f"({count})"
    
    def generate_filename(self, product_name: str, image_index: int, ext: str = "jpg") -> str:
        """
        ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
        
        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•åº”è¯¥åœ¨å¤„ç†å®Œä¸€ä¸ªå•†å“çš„æ‰€æœ‰å›¾ç‰‡åè°ƒç”¨ä¸€æ¬¡ get_suffix
        ç„¶åç”¨è¿”å›çš„åç¼€ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡æ–‡ä»¶å
        
        Args:
            product_name: å•†å“æ ‡é¢˜ï¼ˆåŸå§‹ï¼‰
            image_index: å›¾ç‰‡åºå·ï¼ˆä» 1 å¼€å§‹ï¼‰
            ext: æ–‡ä»¶æ‰©å±•å
            
        Returns:
            å®Œæ•´æ–‡ä»¶åï¼Œå¦‚ "poster-1.jpg" æˆ– "poster-1(1).jpg"
        """
        safe_name = sanitize_filename(product_name)
        suffix = self.get_suffix(safe_name) if image_index == 1 else self._last_suffix
        
        # ä¿å­˜åç¼€ä¾›åŒä¸€å•†å“çš„å…¶ä»–å›¾ç‰‡ä½¿ç”¨
        if image_index == 1:
            self._last_suffix = suffix
        
        return f"{safe_name}-{image_index}{suffix}.{ext}"


def download_images_to_section(
    images: List[str], 
    product_name: str, 
    output_dir: Path,
    name_tracker: ImageNameTracker,
    image_selection: List[int] = None,
    filter_words: List[str] = None
) -> int:
    """
    ä¸‹è½½å•†å“å›¾ç‰‡åˆ° Section ç›®å½•
    
    Args:
        images: å›¾ç‰‡ URL åˆ—è¡¨
        product_name: å•†å“åç§°
        output_dir: è¾“å‡ºç›®å½•
        name_tracker: æ–‡ä»¶åè·Ÿè¸ªå™¨
        image_selection: è¦ä¸‹è½½çš„å›¾ç‰‡åºå·åˆ—è¡¨ï¼ˆ1-indexedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        filter_words: ä»æ ‡é¢˜ä¸­è¿‡æ»¤çš„è¯æ±‡åˆ—è¡¨
        
    Returns:
        æˆåŠŸä¸‹è½½çš„å›¾ç‰‡æ•°é‡
    """
    if not images:
        return 0
    
    # åº”ç”¨æ ‡é¢˜è¿‡æ»¤
    try:
        from .utils import filter_title
    except ImportError:
        from utils import filter_title
    display_name = product_name
    if filter_words:
        display_name = filter_title(product_name, filter_words)
    
    safe_name = sanitize_filename(display_name)
    suffix = name_tracker.get_suffix(safe_name)
    
    # ç¡®å®šè¦ä¸‹è½½çš„å›¾ç‰‡
    if image_selection:
        valid_indices = [i for i in image_selection if 1 <= i <= len(images)]
        skipped_indices = [i for i in image_selection if i > len(images)]
        
        if skipped_indices:
            print(f"    âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„åºå·: {skipped_indices}")
        
        if not valid_indices:
            print("    âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡åºå·")
            return 0
        
        download_list = [(i, images[i-1]) for i in valid_indices]
    else:
        download_list = [(i+1, url) for i, url in enumerate(images)]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
        "Referer": "https://www.etsy.com/"
    }
    
    downloaded = 0
    for idx, url in download_list:
        try:
            # è·å–æ–‡ä»¶æ‰©å±•å
            ext = url.split('.')[-1].split('?')[0] or 'jpg'
            if ext not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                ext = 'jpg'
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"{safe_name}-{idx}{suffix}.{ext}"
            filepath = output_dir / filename
            
            # ä¸‹è½½å›¾ç‰‡
            resp = requests.get(url, headers=headers, timeout=30)
            if resp.status_code == 200:
                filepath.write_bytes(resp.content)
                print(f"    âœ“ {filename}")
                downloaded += 1
            else:
                print(f"    âœ— {filename} (HTTP {resp.status_code})")
                
        except Exception as e:
            print(f"    âœ— å›¾ç‰‡ {idx} ä¸‹è½½å¤±è´¥: {e}")
        
        # çŸ­æš‚å»¶è¿Ÿ
        time.sleep(random.uniform(0.2, 0.5))
    
    return downloaded


def process_product(driver, listing_id: str, output_dir: Path, name_tracker: ImageNameTracker,
                    image_selection: List[int] = None, filter_words: List[str] = None) -> bool:
    """
    å¤„ç†å•ä¸ªå•†å“ï¼šå¯¼èˆªã€æå–æ•°æ®ã€ä¸‹è½½å›¾ç‰‡
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        listing_id: å•†å“ ID
        output_dir: è¾“å‡ºç›®å½•
        name_tracker: æ–‡ä»¶åè·Ÿè¸ªå™¨
        image_selection: è¦ä¸‹è½½çš„å›¾ç‰‡åºå·åˆ—è¡¨
        filter_words: æ ‡é¢˜è¿‡æ»¤è¯åˆ—è¡¨
        
    Returns:
        æ˜¯å¦æˆåŠŸå¤„ç†
    """
    product_url = f"https://www.etsy.com/listing/{listing_id}"
    
    try:
        # å¯¼èˆªåˆ°å•†å“é¡µé¢
        driver.get(product_url)
        time.sleep(random.uniform(2, 4))  # éšæœºå»¶è¿Ÿ
        
        # ä½¿ç”¨ real_chrome_scraper çš„æ•°æ®æå–å‡½æ•°
        # ä½†æˆ‘ä»¬éœ€è¦è·³è¿‡éªŒè¯æ£€æµ‹ï¼ˆå› ä¸ºå·²ç»åœ¨ Section é¡µé¢éªŒè¯è¿‡äº†ï¼‰
        data = extract_product_data_silent(driver)
        
        if not data or not data.get('title'):
            print(f"    âš ï¸ æ— æ³•æå–å•†å“æ•°æ®")
            return False
        
        # ä¸‹è½½å›¾ç‰‡
        images = data.get('images', [])
        if images:
            downloaded = download_images_to_section(
                images, 
                data['title'], 
                output_dir, 
                name_tracker,
                image_selection=image_selection,
                filter_words=filter_words
            )
            total_to_download = len(image_selection) if image_selection else len(images)
            print(f"    â†’ ä¸‹è½½äº† {downloaded}/{total_to_download} å¼ å›¾ç‰‡")
            return downloaded > 0
        else:
            print(f"    âš ï¸ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
            return False
            
    except Exception as e:
        print(f"    âœ— å¤„ç†å¤±è´¥: {e}")
        return False


def extract_product_data_silent(driver) -> Optional[Dict]:
    """
    é™é»˜æå–å•†å“æ•°æ®ï¼ˆä¸æ˜¾ç¤ºéªŒè¯æç¤ºï¼‰
    è¿™æ˜¯ extract_data_with_selenium çš„ç®€åŒ–ç‰ˆæœ¬
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        
    Returns:
        å•†å“æ•°æ®å­—å…¸
    """
    from selenium.webdriver.common.by import By
    
    data = {}
    
    # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨
    for _ in range(2):
        scroll_distance = random.randint(200, 400)
        driver.execute_script(f"window.scrollBy(0, {scroll_distance})")
        time.sleep(random.uniform(0.3, 0.8))
    driver.execute_script("window.scrollTo(0, 0)")
    time.sleep(0.5)
    
    # æå–æ ‡é¢˜
    try:
        title_el = driver.find_element(By.CSS_SELECTOR, 'h1[data-buy-box-listing-title="true"]')
        data['title'] = title_el.text.strip()
    except:
        try:
            title_el = driver.find_element(By.TAG_NAME, 'h1')
            data['title'] = title_el.text.strip()
        except:
            data['title'] = None
    
    # æå–å›¾ç‰‡ - ä½¿ç”¨ä¼˜åŒ–åçš„æ–¹æ³•
    images = []
    seen_ids = set()
    
    def extract_image_id(url):
        match = re.search(r'/il_[^.]+\.(\d+)_', url)
        return match.group(1) if match else None
    
    def convert_to_fullsize(url):
        return re.sub(r'il_[^.]+\.', 'il_fullxfull.', url)
    
    # æ–¹æ³•0: data-src-zoom-imageï¼ˆæœ€ä¼˜å…ˆï¼‰
    try:
        zoom_imgs = driver.find_elements(
            By.CSS_SELECTOR, 
            'li[data-carousel-pane]:not([data-video-pane]) img[data-src-zoom-image]'
        )
        for img in zoom_imgs:
            zoom_url = img.get_attribute('data-src-zoom-image')
            if zoom_url and 'etsystatic.com' in zoom_url:
                img_id = extract_image_id(zoom_url)
                if img_id and img_id not in seen_ids:
                    seen_ids.add(img_id)
                    images.append(zoom_url)
    except:
        pass
    
    # æ–¹æ³•1: ç”»å»ŠåŒºåŸŸï¼ˆå¤‡é€‰ï¼‰
    if not images:
        gallery_selectors = [
            'div[data-component="listing-page-image-carousel"] img',
            'ul[data-carousel-pagination-list] img',
            'ul.carousel-pane-list img[src*="il_"]',
        ]
        
        for selector in gallery_selectors:
            try:
                gallery_imgs = driver.find_elements(By.CSS_SELECTOR, selector)
                for img in gallery_imgs:
                    src = img.get_attribute('src') or img.get_attribute('data-src')
                    if src and 'il_' in src and 'etsystatic.com' in src:
                        img_id = extract_image_id(src)
                        if img_id and img_id not in seen_ids:
                            seen_ids.add(img_id)
                            images.append(convert_to_fullsize(src))
                if images:
                    break
            except:
                continue
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    data['images'] = list(dict.fromkeys(images))[:15]
    
    return data


def process_all_products(
    driver, 
    listing_ids: List[str], 
    output_dir: Path,
    delay: float = 2.0,
    image_selection: List[int] = None,
    filter_words: List[str] = None,
    progress: ScrapeProgress = None
) -> Tuple[int, int]:
    """
    æ‰¹é‡å¤„ç†æ‰€æœ‰å•†å“
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        listing_ids: å•†å“ ID åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        delay: å•†å“é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
        image_selection: è¦ä¸‹è½½çš„å›¾ç‰‡åºå·åˆ—è¡¨
        filter_words: æ ‡é¢˜è¿‡æ»¤è¯åˆ—è¡¨
        progress: è¿›åº¦ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        Tuple[æˆåŠŸæ•°, å¤±è´¥æ•°]
    """
    total = len(listing_ids)
    success_count = 0
    fail_count = 0
    name_tracker = ImageNameTracker()
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹å¤„ç† {total} ä¸ªå•†å“")
    if image_selection:
        print(f"å›¾ç‰‡é€‰æ‹©: {image_selection}")
    if filter_words:
        print(f"æ ‡é¢˜è¿‡æ»¤: {filter_words}")
    print(f"{'='*60}")
    
    for i, listing_id in enumerate(listing_ids, 1):
        print(f"\n[{i}/{total}] å•†å“ ID: {listing_id}")
        
        if process_product(driver, listing_id, output_dir, name_tracker,
                          image_selection=image_selection, filter_words=filter_words):
            success_count += 1
            # æˆåŠŸåç«‹å³ä¿å­˜è¿›åº¦
            if progress:
                progress.save(listing_id)
        else:
            fail_count += 1
        
        # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
        if i < total:
            wait_time = delay + random.uniform(-0.5, 1.0)
            wait_time = max(1.0, wait_time)  # è‡³å°‘ç­‰å¾… 1 ç§’
            print(f"    â³ ç­‰å¾… {wait_time:.1f} ç§’...")
            time.sleep(wait_time)
    
    return success_count, fail_count


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡æŠ“å– Etsy åº—é“º Section ä¸‹çš„æ‰€æœ‰å•†å“å›¾ç‰‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•ä¸ª Section
  etsy-section "https://www.etsy.com/shop/JayDeeDesignShop?section_id=52625173"
  
  # å¤šä¸ª Sectionï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
  etsy-section "https://www.etsy.com/shop/Shop1?section_id=111" "https://www.etsy.com/shop/Shop2?section_id=222"
  
  # å¸¦é€‰é¡¹
  etsy-section "https://www.etsy.com/shop/MyShop?section_id=12345" --output my_images
  etsy-section "https://www.etsy.com/shop/MyShop?section_id=12345" --delay 3

æ–­ç‚¹ç»­ä¼ :
  - é»˜è®¤å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œä¸­æ–­åé‡æ–°è¿è¡Œä¼šè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„å•†å“
  - ä½¿ç”¨ --no-resume ä»å¤´å¼€å§‹ï¼Œå¿½ç•¥ä¹‹å‰çš„è¿›åº¦
  - ä½¿ç”¨ --clear-progress æ¸…ç†è¿›åº¦æ–‡ä»¶åé€€å‡º

å·¥ä½œæµç¨‹:
  1. å¯åŠ¨ Chrome å¹¶æ‰“å¼€ Section é¡µé¢
  2. ä½ æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
  3. æŒ‰ Enter å¼€å§‹è‡ªåŠ¨æŠ“å–
  4. è‡ªåŠ¨éå†æ‰€æœ‰å•†å“å¹¶ä¸‹è½½å›¾ç‰‡ï¼ˆå¤šé“¾æ¥ä¼šä¾æ¬¡å¤„ç†ï¼‰
"""
    )
    
    parser.add_argument("urls", nargs="+", help="Etsy Section URLï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    parser.add_argument("--output", "-o", default="output", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰")
    parser.add_argument("--port", "-p", type=int, default=9222, help="Chrome è°ƒè¯•ç«¯å£ï¼ˆé»˜è®¤: 9222ï¼‰")
    parser.add_argument("--delay", "-d", type=float, default=2.0, help="å•†å“é—´å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 2ï¼‰")
    parser.add_argument("--section-delay", type=float, default=3.0, help="Section é—´å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 3ï¼‰")
    parser.add_argument("--images", "-i", default=None,
                        help="æŒ‡å®šä¸‹è½½å“ªäº›å›¾ç‰‡ï¼Œå¦‚: '1' æˆ– '1,3,5' æˆ– '2-4' æˆ– '1,3-5,8'")
    parser.add_argument("--filter", "-f", default=None,
                        help="ä»æ ‡é¢˜ä¸­è¿‡æ»¤çš„è¯æ±‡ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚: 'Canvas,Poster,Wall Art'")
    
    # æ–­ç‚¹ç»­ä¼ å‚æ•°
    resume_group = parser.add_mutually_exclusive_group()
    resume_group.add_argument("--resume", dest="resume", action="store_true", default=True,
                              help="å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤ï¼‰")
    resume_group.add_argument("--no-resume", dest="resume", action="store_false",
                              help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œä»å¤´å¼€å§‹æŠ“å–")
    parser.add_argument("--clear-progress", action="store_true",
                        help="æ¸…ç†è¿›åº¦æ–‡ä»¶åé€€å‡º")
    
    args = parser.parse_args()
    
    # è§£æå›¾ç‰‡é€‰æ‹©å’Œè¿‡æ»¤è¯å‚æ•°
    try:
        from .utils import parse_image_selection, parse_filter_words
    except ImportError:
        from utils import parse_image_selection, parse_filter_words
    
    image_selection = None
    filter_words = None
    
    if args.images:
        try:
            image_selection = parse_image_selection(args.images)
        except ValueError as e:
            print(f"\nâŒ {e}")
            sys.exit(1)
    
    if args.filter:
        filter_words = parse_filter_words(args.filter)
    
    # éªŒè¯æ‰€æœ‰ URL å¹¶è§£æ section ä¿¡æ¯
    sections = []
    for url in args.urls:
        try:
            shop_name, section_id = parse_section_url(url)
            sections.append({
                'url': url,
                'shop_name': shop_name,
                'section_id': section_id
            })
        except ValueError as e:
            print(f"\nâŒ æ— æ•ˆçš„ Section URL: {url}")
            print(f"   {e}")
            sys.exit(1)
    
    total_sections = len(sections)
    
    print(f"\nâœ“ è§£ææˆåŠŸ: {total_sections} ä¸ª Section")
    for i, s in enumerate(sections, 1):
        print(f"  [{i}] {s['shop_name']} (Section: {s['section_id']})")
    
    # å¤„ç† --clear-progress å‚æ•°
    if args.clear_progress:
        cleared = 0
        output_base = Path(args.output)
        for s in sections:
            target_section_id = s['section_id']
            found = False
            # æ‰«ææ‰€æœ‰å­ç›®å½•ï¼ŒæŸ¥æ‰¾åŒ¹é… section_id çš„è¿›åº¦æ–‡ä»¶
            if output_base.exists():
                for subdir in output_base.iterdir():
                    if subdir.is_dir():
                        progress_file = subdir / ".progress.json"
                        if progress_file.exists():
                            try:
                                with open(progress_file, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                if data.get('section_id') == target_section_id:
                                    progress_file.unlink()
                                    print(f"âœ“ å·²æ¸…ç†: {progress_file}")
                                    cleared += 1
                                    found = True
                            except Exception:
                                pass
            if not found:
                print(f"âš ï¸ æœªæ‰¾åˆ° Section {target_section_id} çš„è¿›åº¦æ–‡ä»¶")
        if cleared == 0:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¿›åº¦æ–‡ä»¶")
        else:
            print(f"\nâœ“ å…±æ¸…ç† {cleared} ä¸ªè¿›åº¦æ–‡ä»¶")
        sys.exit(0)
    
    # æ˜¾ç¤ºè¿‡æ»¤é€‰é¡¹
    if image_selection:
        print(f"  å›¾ç‰‡é€‰æ‹©: {image_selection}")
    if filter_words:
        print(f"  æ ‡é¢˜è¿‡æ»¤: {filter_words}")
    
    print("\n" + "=" * 60)
    print("ğŸ›ï¸  ETSY SECTION SCRAPER")
    print("=" * 60)
    print("æ‰¹é‡æŠ“å–åº—é“º Section ä¸‹çš„æ‰€æœ‰å•†å“å›¾ç‰‡")
    if total_sections > 1:
        print(f"\nğŸ“‹ å…± {total_sections} ä¸ª Section å¾…å¤„ç†")
    print("=" * 60)
    
    # æ­¥éª¤ 1ï¼šå¯åŠ¨ Chrome
    print("\nğŸ“Œ æ­¥éª¤ 1: å¯åŠ¨ Chrome")
    print("-" * 40)
    print("âš ï¸  è¯·å…ˆå…³é—­æ‰€æœ‰ Chrome çª—å£ï¼")
    input("å‡†å¤‡å¥½åæŒ‰ Enter ç»§ç»­...")
    
    print("\nå¯åŠ¨ Chrome...")
    chrome_process = start_chrome_with_debug(sections[0]['url'], args.port)
    
    print("ç­‰å¾…æµè§ˆå™¨å°±ç»ª...")
    if not wait_for_chrome_ready(args.port):
        print("âŒ Chrome å¯åŠ¨å¤±è´¥ï¼")
        chrome_process.terminate()
        sys.exit(1)
    
    print("âœ“ Chrome å·²å¯åŠ¨ï¼")
    
    # æ­¥éª¤ 2ï¼šç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯
    print("\n" + "=" * 60)
    print("ğŸ“Œ æ­¥éª¤ 2: å®ŒæˆéªŒè¯")
    print("-" * 40)
    print("""
åœ¨æ‰“å¼€çš„ Chrome çª—å£ä¸­ï¼š

  1. å¦‚æœçœ‹åˆ°éªŒè¯é¡µé¢ï¼Œè¯·å®Œæˆã€Œæˆ‘ä¸æ˜¯æœºå™¨äººã€éªŒè¯
  2. ç­‰å¾… Section é¡µé¢å®Œå…¨åŠ è½½
  3. ç¡®è®¤èƒ½çœ‹åˆ°å•†å“åˆ—è¡¨

â° æ²¡æœ‰æ—¶é—´é™åˆ¶ï¼Œæ…¢æ…¢æ¥ï¼
""")
    print("=" * 60)
    
    input("\nâœ‹ éªŒè¯å®Œæˆã€é¡µé¢åŠ è½½å¥½åï¼ŒæŒ‰ Enter ç»§ç»­...")
    
    # è¿æ¥åˆ° Chrome
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    
    options = Options()
    options.add_experimental_option("debuggerAddress", f"localhost:{args.port}")
    driver = webdriver.Chrome(options=options)
    
    # ç»Ÿè®¡
    total_success = 0
    total_fail = 0
    total_skipped = 0
    sections_completed = 0
    
    try:
        for sec_idx, section in enumerate(sections, 1):
            url = section['url']
            shop_name = section['shop_name']
            section_id = section['section_id']
            
            if total_sections > 1:
                print(f"\n{'='*60}")
                print(f"[Section {sec_idx}/{total_sections}] {shop_name}")
                print(f"  Section ID: {section_id}")
                print(f"{'='*60}")
            
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ª Sectionï¼Œéœ€è¦å¯¼èˆªåˆ°æ–°é¡µé¢
            if sec_idx > 1:
                try:
                    driver.get(url)
                    time.sleep(2)
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    for _ in range(2):
                        scroll_distance = random.randint(200, 400)
                        driver.execute_script(f"window.scrollBy(0, {scroll_distance})")
                        time.sleep(random.uniform(0.3, 0.8))
                    driver.execute_script("window.scrollTo(0, 0)")
                    time.sleep(1)
                except Exception as e:
                    print(f"  âŒ å¯¼èˆªå¤±è´¥: {e}")
                    continue
            
            # è·å– Section ä¿¡æ¯ï¼ˆåœ¨åˆ›å»ºè¾“å‡ºç›®å½•ä¹‹å‰è·å– section åç§°ï¼‰
            print(f"\n  ğŸ“Œ è·å– Section ä¿¡æ¯...")
            section_name, total_items = get_section_info(driver, section_id)
            print(f"    Section: {section_name}")
            print(f"    é¢„è®¡å•†å“æ•°: {total_items}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨ section å®é™…åç§°ï¼‰
            if section_name and section_name != "section":
                section_dir_name = sanitize_folder_name(section_name)
            else:
                section_dir_name = f"{shop_name}_{section_id}"
            
            # åŒåæ–‡ä»¶å¤¹å†²çªæ£€æµ‹
            candidate_path = Path(args.output) / section_dir_name
            if candidate_path.exists():
                progress_file = candidate_path / ".progress.json"
                if progress_file.exists():
                    try:
                        with open(progress_file, 'r', encoding='utf-8') as f:
                            existing_progress = json.load(f)
                        if existing_progress.get('section_id') != section_id:
                            section_dir_name = f"{section_dir_name}_{section_id}"
                    except Exception:
                        pass
            
            output_path = Path(args.output) / section_dir_name
            output_path.mkdir(parents=True, exist_ok=True)
            print(f"  è¾“å‡ºç›®å½•: {output_path}")
            
            # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
            progress = ScrapeProgress(output_path, url, shop_name, section_id)
            
            # åŠ è½½å·²æœ‰è¿›åº¦ï¼ˆå¦‚æœå¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼‰
            completed_ids = set()
            if args.resume:
                try:
                    completed_ids = progress.load()
                    if completed_ids:
                        print(f"  ğŸ“‹ æ£€æµ‹åˆ°è¿›åº¦ï¼šå·²å®Œæˆ {len(completed_ids)} ä¸ªå•†å“")
                except ValueError as e:
                    print(f"  âŒ {e}")
                    continue
            
            # æå–å•†å“é“¾æ¥
            print(f"\n  ğŸ“Œ æå–å•†å“é“¾æ¥...")
            
            # æå–æ‰€æœ‰å•†å“é“¾æ¥ï¼ˆä¼ å…¥ total_items ç”¨äºè®¡ç®—ç¿»é¡µï¼‰
            listing_ids = extract_product_links(driver, url, total_items=total_items)
            
            if not listing_ids:
                print(f"\n  âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å•†å“ï¼")
                continue
            
            print(f"\n  âœ“ å…±æ‰¾åˆ° {len(listing_ids)} ä¸ªå•†å“")
            
            # è®¾ç½®æ€»å•†å“æ•°
            progress.set_total_found(len(listing_ids))
            
            # è¿‡æ»¤å·²å®Œæˆçš„å•†å“
            pending_ids = listing_ids
            skipped_count = 0
            if args.resume and completed_ids:
                pending_ids = [lid for lid in listing_ids if lid not in completed_ids]
                skipped_count = len(listing_ids) - len(pending_ids)
                if skipped_count > 0:
                    print(f"\n  ğŸ“‹ æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡ {skipped_count} ä¸ªå·²å®Œæˆå•†å“")
                    total_skipped += skipped_count
            
            if not pending_ids:
                print(f"\n  âœ“ æ‰€æœ‰å•†å“å·²å®Œæˆï¼")
                sections_completed += 1
                continue
            
            # ç¡®è®¤ç»§ç»­ï¼ˆåªæœ‰å•ä¸ª Section æ—¶è¯¢é—®ï¼‰
            if total_sections == 1:
                print("\n" + "=" * 60)
                confirm = input(f"æ˜¯å¦å¼€å§‹ä¸‹è½½ {len(pending_ids)} ä¸ªå•†å“çš„å›¾ç‰‡? (Y/n): ")
                if confirm.lower() == 'n':
                    print("å·²å–æ¶ˆ")
                    chrome_process.terminate()
                    sys.exit(0)
            
            # å¤„ç†å•†å“
            print(f"\n  ğŸ“Œ ä¸‹è½½å•†å“å›¾ç‰‡ ({len(pending_ids)} ä¸ª)...")
            
            success, fail = process_all_products(
                driver, 
                pending_ids, 
                output_path,
                delay=args.delay,
                image_selection=image_selection,
                filter_words=filter_words,
                progress=progress
            )
            
            total_success += success
            total_fail += fail
            
            # Section å®ŒæˆçŠ¶æ€
            if progress.completed_count == len(listing_ids):
                sections_completed += 1
                print(f"\n  âœ“ Section å®Œæˆï¼")
            else:
                print(f"\n  ğŸ“‹ è¿›åº¦: {progress.completed_count}/{len(listing_ids)} å®Œæˆ")
            
            # Section é—´å»¶è¿Ÿ
            if sec_idx < total_sections:
                wait_time = args.section_delay + random.uniform(-0.5, 1.0)
                wait_time = max(1.0, wait_time)
                print(f"\nâ³ ç­‰å¾… {wait_time:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ª Section...")
                time.sleep(wait_time)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ å®Œæˆï¼")
        print("=" * 60)
        if total_sections > 1:
            print(f"  Section æ€»æ•°: {total_sections}")
            print(f"  å®Œæˆçš„ Section: {sections_completed}")
        print(f"  å•†å“æˆåŠŸ: {total_success}")
        print(f"  å•†å“å¤±è´¥: {total_fail}")
        if total_skipped > 0:
            print(f"  å•†å“è·³è¿‡ (æ–­ç‚¹ç»­ä¼ ): {total_skipped}")
        print(f"  è¾“å‡ºç›®å½•: {args.output}")
        
    finally:
        # è¯¢é—®æ˜¯å¦å…³é—­æµè§ˆå™¨
        close = input("\næ˜¯å¦å…³é—­ Chrome æµè§ˆå™¨? (y/N): ")
        if close.lower() == 'y':
            chrome_process.terminate()
            print("æµè§ˆå™¨å·²å…³é—­")
        else:
            print("æµè§ˆå™¨ä¿æŒæ‰“å¼€")


if __name__ == "__main__":
    main()
